{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEw8HcMTn7-V"
      },
      "source": [
        "# Phi-4 MultiModal - Tested!\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/designingwithml/blogposts/blob/main/notebooks/modelevals/phi4/phi-4-multimodal.ipynb)\n",
        "\n",
        "\n",
        "> Note: Phi 4 is released with a specific dependency list.  Install them in order\n",
        "\n",
        "```bash\n",
        "torch==2.6.0\n",
        "flash_attn==2.7.4.post1\n",
        "transformers==4.48.2\n",
        "accelerate==1.3.0\n",
        "soundfile==0.13.1\n",
        "pillow==11.1.0\n",
        "scipy==1.15.2\n",
        "torchvision==0.21.0\n",
        "backoff==2.2.1\n",
        "peft==0.13.2\n",
        "```\n",
        "\n",
        "Also using a virtual environment is recommended. Conda or venv will work.\n",
        "\n",
        "We will cover:\n",
        "\n",
        "- Text generation\n",
        "- Image decription/captioning â€¦\n",
        "- Audio transcription, text translation\n",
        "- Function Calling\n",
        "- OCR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoAkir3vn7-X"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.6.0 transformers==4.48.2 accelerate==1.3.0 soundfile==0.13.1 pillow==11.1.0 scipy==1.15.2 torchvision==0.21.0 backoff==2.2.1 peft==0.13.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash_attn==2.7.4.post1 --verbose"
      ],
      "metadata": {
        "id": "GiF0X7-sxtF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pc9GwBkn7-Y"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import os\n",
        "import io\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the model and processor\n",
        "# model_path = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "# processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_path,\n",
        "#     device_map=\"cuda\",\n",
        "#     torch_dtype=\"auto\",\n",
        "#     trust_remote_code=True,\n",
        "#     attn_implementation='flash_attention_2',\n",
        "# ).cuda()\n",
        "# generation_config = GenerationConfig.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "XGJb42Wq7oiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.phi_4_multimodal import run_phi4_multimodal"
      ],
      "metadata": {
        "id": "7ji6ZtxJzEI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "model_id = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "data_manifest = \"datasets/commonvoice_test_mdc.json\"\n",
        "data_folder = \"data/commonvoice\"\n",
        "output_manifest = \"outputs/phi4_multimodal_commonvoice.json\""
      ],
      "metadata": {
        "id": "kxwGhL8xzE_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256"
      ],
      "metadata": {
        "id": "7CjCiJGQzsqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation parameters\n",
        "max_new_tokens = 256  # Maximum tokens to generate per transcription\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Running {model_id} with Batch Processing\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Input manifest: {data_manifest}\")\n",
        "print(f\"Output manifest: {output_manifest}\")\n",
        "print(f\"Auto batch size: {batch_size}\")\n",
        "\n",
        "print(f\"Max new tokens: {max_new_tokens}\")\n",
        "print(f\"{'='*60}\\n\")"
      ],
      "metadata": {
        "id": "cgyKV6T1z0qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9746c1e7cbd2223cbe05a58cd9bc99b740c9e2ac9e73a7996dc029fbd3009118"
      ],
      "metadata": {
        "id": "vxftPbZM0DCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_phi4_multimodal(\n",
        "            model_id=model_id,\n",
        "            data_manifest=data_manifest,\n",
        "            data_folder=data_folder,\n",
        "            output_manifest=output_manifest,\n",
        "            batch_size=batch_size,\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )"
      ],
      "metadata": {
        "id": "rykNXNLd6AZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "phi4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}